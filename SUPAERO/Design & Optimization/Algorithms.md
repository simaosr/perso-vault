# Gradient-free vs Gradient-based
- Gradient-free will be easier to setup at an initial phase.
- Gradient-based will be much more efficient.
	- The problem of a noisy adjoint solution can be addressed using Gradient-Enhanced Krigging
	- The adjoint of the body force model must be possible to obtain in order to use GB algorithms.


**(Benchmarking Optimization Algorithms for Wing Aerodynamic Design Optimization, 2014)**
> We evaluated several optimization algorithms for three different aerodynamic shape optimization problems.The algorithms we considered included gradient-based methods with adjoint gradients and gradient-freemethods (a particle swarm optimization and a genetic algorithm). The gradient-free methods required 2 to 4 orders of magnitude more iterations than gradient-based methods. ==We conclude that gradient-basedmethods with adjoint gradients are the best choice for solving large-scale aerodynamic design optimization problems.==

**(On the Influence of Optimization Algorithm andInitial Design on Wing Aerodynamic Shape Optimization, 2018)**
> All the optimizers except NSGA2 converged to the reference optimum, with identicaltwist distributions and only a 0.1 difference in the drag counts. ==The gradient-free methods were several  times  more  expensive  than  the  gradient-based  methods,  ranging  from  four  times  more expensive  (NOMAD)  to  over  100  (NSGA2).== Thus,  gradient-based  methods  are  a  better  choice because they require many fewer CFD evaluations and are equally robust. We also benchmarked four of the gradient-based algorithms separately for a higher dimensional problem (192 design variables including twist and airfoil shape) using a finer grid. All the optimizers converged to the same optimum, reducing the drag by 4.84%, from 206.7 to 196.6 counts.  SNOPT converged the fastest, using just under 225 processor-hours.

# Gradient Free
## NSGA-II
> NSGA2 is a non-dominant sorting based multi-objective evolutionary algorithm. The optimizer enforces constraints by tournament selection. It can solve non-smooth and non-convex multi-objective functions and tends to approach the global minimum

## ALPSO
> ALPSO is a parallel augmented Lagrange multiplier particle swarm optimization (PSO) solver written in Python. This method takes advantage of PSO methods, which can solve non-smooth objective func-tions and is more likely to find the global minimum. Augmented Lagrange multipliers are used to handleconstraints. ALPSO can be used for nonlinear, non-differentiable, and non-convex problems. Perez and Behdinan applied this method to a non-convex, constrained structural problem. Other applications include aerostructural optimization of nonplanar lifting surfaces and aeroservoelastic design optimization of aflexible wing.

## NOMAD
> NOMAD is a C++ implementation of the mesh adaptive direct search (MADS) algorithm Audetet al. (2006) for blackbox optimization with nonlinear constraints Le Digabel et al. (2012); Le Digabel (2011).  It performs an adaptive search on a tower of underlying meshes to find abetter solution and also controls the refinement of the meshes.

# Gradient Based

## SNOPT
> SNOPT is a sequential quadratic programming (SQP) method that uses a smooth augmented Lagrangian merit function and reduced-Hessian semi-definite QP solver for the QP subproblems. It solves large-scale problems with nonlinear constraints and a smooth objective.

- Website: https://web.stanford.edu/group/SOL/snopt.htm
- Might be dificult to use as non-US

## SLSQP
> SLSQP is a sequential least squares programming algorithm that evolved from the least squares solver of Lawson and Hanson. The optimizer uses a quasi-Newton Hessian approximation and an L1-test function in the line search algorithm. Kraft also applied this method to aerodynamic and robotic trajectory optimization.

## IPOPT
> IPOPT implements a primal-dual interior-point algorithm with a filter line search method. The barrier problem is solved using a damped Newtonâ€™s method. The line search method includes a second ordercorrection.

- Website: https://github.com/coin-or/Ipopt


## PSQP 
> PSQP is a preconditioned SQP method with a BFGS variable metric update. It can handle large scale problems with nonlinear constraints.

## CONMIN 
> CONMIN solves linear or nonlinear optimization problems using the method of feasible directions. It minimizes the objective function until it reaches an infeasible region. The optimization then continues by following the constraint boundaries in a descent direction.

## GCMMA 
> GCMMA is a modified version of the method of moving asymptotes, designed for nonlinear programmingand structural optimization. It solves a strictly convex approximating sub-problem at each iteration. GCMMA guarantees convergence to a local minimum from any feasible starting point.

